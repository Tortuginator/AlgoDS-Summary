\chapter{Information and Compression}
\section{Probability Theory}
Recall that a probability space consists of a set $\Omega$\mathmarginbox{$\Omega$}, the \highlight{sample space}, a set $\mathcal{E} \subseteq 2^{\Omega}$ , the \highlight{event space}, and a \symbdefbox{probability distribution}{$\mathcal{P}$} that associates an probability $\mathcal{P}(E)$ with each event $E \in \mathcal{E}$.

For an event $A(\omega)$ depending on an element $\omega \in \Omega$ of the sample space we sometimes write $\operatorname{Pr}_{w\sim\mathcal{P}}(A(\omega))$ to denote the probability $\mathcal{P}(A(\omega))$

A \highlight{random variable} on a probability space $(\Omega,\mathcal{P})$ is a mapping $X: \Omega \rightarrow \mathbb{R}$. By $X < x$, for  $x \in \mathbb{R}$, we done the event $\{\omega \in \Omega \,|\, X(\omega) \leq x\}$. We usually denote the probability of this event by $\operatorname{Pr}(X \leq x)$, without reference to the probability distribution $\mathcal{P}$.

The \symbdefbox{cumulative distribution function}{$F_X$} of $X$ is defined by $F_X:\mathbb{R} \rightarrow [0,1]$.

For random variables $X$ with a finite or countably infinite range, we can define the \highlight{probability mass function} $f_X:\mathbb{R} \rightarrow [0,1]$ by $f_X(x) := \operatorname{Pr}(X=x)$. Then have
\begin{equation*}
F_X(x)=\sum_{y\leq x}f_x(y)
\end{equation*}
This gives us a \symbdefbox{probability distribution}{$\mathcal{P}_x$} the probability distribution of $X$
\begin{equation*}
\mathcal{P}_X(A) := \sum_{x \in A} \operatorname{Pr}(X = x)
\end{equation*}

For continuous random variables (on an arbitrary probability space) there exists a \highlight{propability density function}
\begin{equation*}
F_X(x) = \int_{-\infty}^{x}f_x(y)dy
\end{equation*}

\subsection{Random Vectors and Joint Distributions}
Let $(X,Y)$ be a pair of random variables with a finite range over the same probability space $(\Omega,\mathcal{P})$. The \highlight{joint distribution of $X,Y$} is the probability distribution $\mathcal{P}_{(X,Y)}$ on $\mathbb{R}^2$ defined by

\begin{equation*}
\begin{aligned} 
\mathcal{P}_{(X, Y)}(\{(x, y)\}):&=\operatorname{Pr}(X=x, Y=y) \\ 
&=\mathcal{P}(\{\omega \in \Omega | X(\omega)=x \text { and } Y(\omega)=y\}) \end{aligned}
\end{equation*}

Conversely, from the joint distribution we can retrieve the \highlight{marginal distribution} of $X$
\begin{equation*}
\operatorname{Pr}(X=x)=\sum_{y} \operatorname{Pr}((X, Y)=(x, y))
\end{equation*}

A tuple $\mathbf{X} = (X_1,\ldots,X_k)$ of random variables is sometimes called a \highlight{random vector}

\subsection{Independence}
Let $(\Omega,P)$ be a probability space and $k \geq 1$. Events $E_1,\ldots,E_k \subseteq$ are \highlight{independent} if $\mathcal{P}\left(E_{1}, \ldots, E_{k}\right)=\mathcal{P}\left(E_{1}\right) \cdots \mathcal{P}\left(E_{k}\right)$. Here $E_1,\ldots,E_k$ denotes the event $E_{1} \cap \ldots \cap E_{k}$.

Random variables $X_1, \ldots, X_k : \Omega \rightarrow \mathbb{R}$ are independent if for all $A_{1}, \ldots, A_{k} \subseteq \mathbb{R}$ the events $X_i \in A_i $ are independent, that is, 
\begin{equation*}
\operatorname{Pr}(X_1 \in A_{1}, \ldots, X_k \in A_k)=\operatorname{Pr}(X_1 \in A_1) \cdots \operatorname{Pr}(X_k \in A_k)
\end{equation*} We also say that the random vector $ \mathbf{X}=(X_1, \ldots, X_k) $ is independent. 

\subsection{Expectation and Variance}
Let $X$ be a random variable. 
The expectation of $x$ is $\mathrm{E}(X)=\sum_{x \in \mathbb{R}} x \cdot \operatorname{Pr}(X=x)$

The variance of $X$ is $\operatorname{Var}(X)=E((X-E(X))^{2})$

\begin{enumerate}
	\item For $\alpha, \beta \in \mathbb{R}, \mathrm{E}(\alpha X+\beta Y)=\alpha \mathrm{E}(X)+\beta \mathrm{E}(Y)$ linearity of expectation
	\item $X$ and $Y$ are independent then $\mathrm{E}(X \cdot Y)=\mathrm{E}(X) \mathrm{E}(Y)$
	\item $\operatorname{Var}(X)=\mathrm{E}\left(X^{2}\right)-\mathrm{E}(X)^{2}$
	\item $X$ and $Y$ are independent then $\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)$
\end{enumerate}

\subsection{Markov's and Chebyshev's Inequalities}
\highlight{Markov's Inequality}. Let $X$ be a nonnegative random variable. Then for all $a \geq 0$, 
\begin{equation}
\operatorname{Pr}(X \geq a) \leq \frac{E(X)}{a}
\end{equation}
\highlight{Chebyshev's Inequality}. Let $X$ be a random variable. Then for all $b > 0$,
\begin{equation}
\operatorname{Pr}(|X-\mathrm{E}(X)| \geq b) \leq \frac{\operatorname{Var}(X)}{b^{2}}
\end{equation}
\begin{proof}[Proof of Markov's Inequality]
	See Slide 2.8-a
\end{proof}
\begin{proof}[Proof of Chebyshev's Inequalit]
	See Slide 2.8-a
\end{proof}
\subsection{k-Wise Independence}
A sequence $X_1,\ldots,X_n$ of random variables is \highlight{k-wise independent}, for some $k \geq n$, if for all $i_1 < \ldots < i_k \in [n]$ the sequence $X_{i_1},\ldots,X_{i_k}$ is independent. Instead 2-wise independent, we usually say \highlight{pairwise independent}

Let $X_1,\ldots,X_n$ be a pairwise independent sequence of random variables and $X := \sum_{i=1}^{n}X_i$. Then 
\begin{equation}
Var(X) = \sum_{i=1}^n \operatorname{Var}(X_i)
\end{equation}

\begin{proof}
	See Slide 2.9-a
\end{proof}
\subsection{The Weak Law of Large Numbers}
TODO
\section{Digression: Concentration Inequalities}
\subsection{Sum of Random Variables}
The sum of a rancom process is follows:
\begin{equation*}
X = \sum_{i=1}^{n}X_i
\end{equation*}
Obviously, the expected value is $\mu := E(X)$. We show, that with high probability, $X$ is close to its expected value, that is: 
\begin{equation*}
\operatorname{Pr}(|X - \mu| \geq \text{something big} ) \leq \text{something small}
\end{equation*}
These inequalities are called \highlight{concentration inequalities} or \highlight{tail bounds}.
\subsection{Chernoff Bounds}
Let $X_1,\ldots,X_n$ be a sequence of independent $\{0,1\}$-valued random variables. Let $X := \sum^n_{i=1}X_i$ and $\mu := E(X)$. Then for $0 \leq c \leq 1$:
\begin{equation*}
\operatorname{Pr}(X \geq (1+c)\mu)\leq e^{-\frac{\mu c^2}{3}}\\
\text{ and } \operatorname{Pr}(X \leq (1-c)\mu)\leq e^{-\frac{\mu c^2}{2}}
\end{equation*}
\subsection{Hoeffding Bounds}
Let $X_1,\ldots,X_n$ be a sequence of independent identically distributed $\{0,1\}$-valued random variables. Let $X := \sum^n_{i=1}X_i$ and $\mu := E(X)$. Then for $0 \leq d \leq 1$:
\begin{equation*}
\operatorname{Pr}(X \geq \mu +dn)\leq e^{-2nd^2}\\
\text{ and } \operatorname{Pr}(X \leq \mu -dn)\leq e^{-2nd^2}
\end{equation*}
\subsection{General Random variable concentration}

\begin{theorem}[General random variable Concentration]{important}
Let $X_1,\ldots,X_n$ be a sequence of independent random variable swit $E(X_i) = 0$ and $\operatorname{Var}(X_i) \leq \sigma^2$, and $X:= \sum^{n}_{i=1}X_i$. Let $a \in \mathbb{R}$ such that $0\leq a\leq \sqrt{2}n\sigma^2$ and suppose that $E(X_i^k) \leq \sigma^2k!$ for $3 \leq k \leq \lceil\frac{a^2}{4n\sigma^2}\rceil$. Then $\operatorname{Pr}(|X| \geq a) \leq 3e^{-\frac{a^2}{12n\sigma^2}}$.
\end{theorem}
\section{Entropy}
The \symbdefbox{information content}{$I(A)$} of a event $A$ in a finite probability space $(\Omega,\mathcal{P})$ is:
\begin{equation*}
I(A) = \operatorname{log}(\frac{1}{\mathcal{P}(A)})
\end{equation*}

\begin{proof}
	Proof and Lemma for the corresponding base of the logarithm on Slide $2.20-(a-c)$
\end{proof}

The \highlight{entropy} of a probability distribution $\mathcal{P}$ on a finite sample space $\Omega$ is defined to be
\begin{equation*}
H(\mathcal{P}) := \sum_{w \in \Omega}\mathcal{P}(\{w\})* \operatorname{log}\frac{1}{\mathcal{P}(\{w\})}
\end{equation*}
where we take $0 \operatorname{log}(1/0)$ to be 0 (or only sum over all $w$ with $\mathcal{P}(w)>0$).

The \highlight{entropy} of a random variable $X$ with finite range is defined to be 
\begin{equation*}
H(X) := \sum_{x\in rg(X)}\operatorname{Pr}(X \, = \, x)* \operatorname{log}\frac{1}{\operatorname{Pr}(X \, = \, x)}
\end{equation*}
%TODO: what is the rg function?
\subsection{Lower and Upper Bounds}
 Let $\mathcal{P}$ be a probability distribution on a sample space $\Omega$ of size $|\Omega|=n$ Then
 \begin{equation*}
 	0 \leq H(\mathcal{P}) \leq \log n
 \end{equation*}
 Furthermore, 
 \begin{equation*}
	H(\mathcal{P})=0 \Longleftrightarrow \exists \omega \in \Omega : \mathcal{P}(\{\omega\})=1
 \end{equation*}
 and
 \begin{equation*}
 \begin{split} 
 H(\mathcal{P})=\log (n) \Longleftrightarrow & \mathcal{P} \text { is the uniform distribution, that } \\ & \text { is, } \mathcal{P}(\{\omega\})=1 / n \text { for all } \omega \in \Omega 
 \end{split}
 \end{equation*}

  Let $f : D \rightarrow \mathbb{R} $ , where $ D \subseteq \mathbb{R} . $ Then $ f $ is convex if for all $ \lambda \in[0,1] $ and all $ x, y \in D $ it holds that $ \lambda x+(1-\lambda) y \in D $ and 
  \begin{equation*}
  f(\lambda x+(1-\lambda) y) \leq \lambda f(x)+(1-\lambda) f(y)
  \end{equation*} 
If $f(\lambda x+(1-\lambda) y<\lambda f(x)+(1-\lambda) f(y)$ for all $ \lambda \in(0,1)$ and all distinct $ x, y \in D $ then $f$ is strictly convex.
\subsubsection{Jensen's Inequality}
Let $X$ be a random variable, and let $ f : D \rightarrow \mathbb{R} \text { be a convex function} \text {with } \operatorname{rg}(X) \subseteq D . \text { Then } E(X) \in D$ and 
\begin{equation*}
f(\mathrm{E}(X)) \leq \mathrm{E}(f(X))
\end{equation*}
$f$ is strictly convex, then equality holds if and only if $X$
\begin{proof}
	Proof on page 2.24-(a-c)
\end{proof}
\subsection{Log Sum Inequality}
For all $ i \in[n]$, let $ p_{i} \in \mathbb{R}_{ \geq 0}$,$ q_{i} \in \mathbb{R}_{>0}$, and let $ p :=\sum_{i} p_{i} $ and $q :=\sum_{i} q_{i}$. Then 
\begin{equation*}
\sum_{i=1}^{n} p_{i} \log (\frac{p_{i}}{q_{i}}) \geq p \log (\frac{p}{q})
\end{equation*}
Moreover, equality holds if and only if $p_i/q_i \, = \, p_j/q_j$ for all $i,j$
\begin{proof}
	Proof on page 2.25-(a-c)
\end{proof}
\subsection{Information Gain \& Entropy}
TODO - Slides are not very understandable -- come up with something better
\section{Compression}
Out intuition behind the entropy of a probability distribution $\mathcal{P}$ is that it measures the average 'Information' of a sample from $\mathcal{P}$, that is, of an elementary event $\{w\}$.
\begin{itemize}
	\item Information is the average number of bit we need to transfer (or store) many conservative samples from a distribution, using the best possible coding scheme for this specific distribution.
	\item Suppose we have a string where the symbols are independently sampled from our distribution. Then the information content of the distribution should measure how well we can compress the string.
\end{itemize}
Strings are compressed over a finite \symbdefbox{source alphabet}{$\Sigma$}. We assume $\Sigma \geq 2$. Compressed strings are encoded in the \highlight{target alphabet} $\{0,1\}$. A \highlight{compression scheme} over $\Sigma$ is a pair $\Gamma = (\operatorname{com}_\Gamma,\operatorname{dec}_\Gamma)$ consisting of
\begin{itemize}
	\item \highlight{compression mapping}\mathmarginbox{$\operatorname{com}_\Gamma$}  $\operatorname{com}_\Gamma\text{ : }\Sigma\rightarrow \{0,1\}^*$ 
	\item \highlight{decompression mapping}\mathmarginbox{$\operatorname{dec}_\Gamma$}  $\operatorname{dec}_\Gamma\text{ : }\{0,1\}^* \rightarrow \Sigma^*$.
\end{itemize} 

The goal is to have a compression scheme, that is ideally \highlight{lossless}, that is $dec(com(\vec{x})) = x$ for all $x \in \Sigma^*$, and guarantees a good compression. 

The \highlight{compression rate} of a scheme $\Gamma$ $\vec{X}$ is $\frac{|com(\vec{x})|}{|\vec{x}|}$. Intuitively, the compression rate if $\Gamma$ is the maximum of the compression rates $\Gamma$ of all strings in $\Gamma^*$. As this maximum does not necessarily exist, we define a compression rate for strings of length $n$ separately for each $n$: the compression rate of $\Gamma$ is the function \mathmarginbox{$\rho_{\Gamma}$}$\rho_{\Gamma} : \mathbb{N} \rightarrow \mathbb{R}$ defined by 
\begin{equation*}
\rho_{\Gamma}(n) :=\max _{x \in \Sigma^{n}} \frac{|\operatorname{com}(x)|}{|x|}
\end{equation*}
