%%% File encoding is ISO-8859-1 (also known as Latin-1)
%%% You can use special characters just like �,� and �
\include{amsmath}
\chapter{Machine Learning Basics}

\section{Definitions}
All the following definitions are specific to 'Machine Learning' and might change throughout the summary.


\highlight{Data} is viewed as a collection of data items (or dataset). A \highlight{data item} is represented by a so-called feature vector. The \highlight{feature vector} basically a list of features and attributes in form of a vector 
\subsection{Domains}
The \symbdefbox{Domain}{$\mathbb{D}$} is a set containing the range of all possible values for a single feature. The \symbdefbox{Instance Space}{$\mathbb{X}$} is the Cartesian product of the domains of all features in the feature vector. The number of features is the dimension of the instance space (or the number of entries in the feature vector). 
\example{The domain space can be something like $\{true,false\}$ or $\mathbb{N}$, aso. The instance space of the domain spaces is then (informally) $\mathbb{D}_1\times \mathbb{D}_2 \times \ldots \times \mathbb{D}_l$}
Assuming $n$ is the number of data items, and $l$ the number of features the whole dataset can be viewed as an $(n \times l )$-matrix over the instance space.

\section{Forms of Learning}
\highlight{Unsupervised learning} tries to detect patterns in data with, hence the name, no feedback, supplied the Learning algorithm. This technique is mostly used for clustering.

\highlight{Reinforcement Learning} tries to find actions that maximise reward. The feedback to the learning algorithm is provided in form of a reward (or punishment)

In \highlight{Supervised learning} one tries to approximate a function by training on input-output pairs. \highlight{Classification} has a finite-valued function and tries to predict values for future inputs. \highlight{Regression} has a numerical function and tries to predict the expected values for future inputs. Supervised learning can be differentiated into two different set-ups. First, \highlight{Batch Learning}, in which all examples are supplied at once and the learner has to come up with a hypothesis. Where as in \highlight{Online Learning}, examples are given 'on-the-fly' once at a time and the initial hypothesis gets improved over time.

\highlight{Semi-supervised Learning} is similar to supervised learning, but takes only a few and possibly faulty examples and tries to make the best of the possibly small and faulty dataset.

%TODO: Add Active/Passive Learning

\section{Hypothesis Space}
In Supervised learning, the unknown target function $h$ is choosen from the \symbdefbox{hypothesis space}{$\mathcal{H}$}, hence $h \in \mathcal{H}$. The Hypothesis space contains all linear functions, all polynomials or all polynomials of a pre-scribed degree, and all functions that can be described by a decision tree.

A learning problem is \highlight{Realisable} if the target function $h$ is in the Hypothesis Space.
\example{Assuming a target function $h(x):= a \cdot \operatorname{sin}(x) + bx + c$. Then, if $\mathcal{H}$ only contains polynomials, the learning problem is \textbf{not realizable}. But, assuming $\mathcal{H}$ contains linear functions or polynomials in $x$ and $\operatorname{sin}(x)$, the learning problem \textbf{is realisable}.}

\section{Validation}
The goal of the learning algorithm is to produce a hypothesis that generalizes well, that is, approximates the target function well ion the all data points (and not only those in the training set). To evaluate how well a hypothesis generalizes we can evaluate it against some test set. A \highlight{test set} is generates by splitting the examples into a test set and training set.

The empirical observation that simpler hypothesis tend to generalise better is picked up by \highlight{Occam's Razor} which states, that the simplest hypothesis which is consistent with the data should be choosen.

\highlight{Overfitting} is the phenomenon, which occurs when the leaner tries to match the training examples as exactly as possible which leads to complex hypotheses that generalize badly. Thus, to avoid overfitting, it is often better to choose a simple hypothesis even if it doesn't match the data exactly.

\section{Nearest Neighbour Learning}
The underlying idea of this simple learning algorithm is to predict the value of a function at point $x$ by looking at known values of points close to the given one and assume the value of $x$ is similar. 
\subsection{Metric}
A \symbdefbox{metric}{$d$} on $\mathbb{X}$ is a function $d: \mathbb{X}^2 \longrightarrow  \mathbb{R}$ such that for all $x,y,z \in \mathbb{X}$

\newlength\q
\setlength\q{\dimexpr .33\textwidth -2\tabcolsep}
\begin{tabular}{p{\q}|p{\q}|p{\q}}
	\highlight{Nonnegativity} & \highlight{Symmetry} & \highlight{Triangle Inequality}\\
	$\begin{aligned} &d(x,y) \geq 0 \\	\texttt{and }  &d(x,y) = 0 \Longleftrightarrow x=y;\end{aligned}$ & 	$d(x,y) = d(y,x)$ &  $d(x,z) \leq d(x,y) + d(y,z)$\\
\end{tabular}


\subsection{Metric Spaces}
Suppose $\mathbb{X}$ is the instance space, and $d$ be some metric on $\mathbb{X}$. Then we have a \symbdefbox{Metric Space}{$(\mathbb{X},d)$}.

\subsubsection{Common distance metrics}
The distance between points $x$ and $y$ is denoted as $d(x,y)$\mathmarginbox{$d(x,y)$}.
\setlength{\columnseprule}{0.4pt}
\begin{multicols}{2}
\highlight{Euclidean distance} with $\mathbb{X} = \mathbb{R}^\ell$:
\begin{equation}
d(x,y) = \sqrt{\sum^{\ell}_{i=1} (x_i - y_i)^2}
\end{equation}

\highlight{Manhattan distance} with $\mathbb{X} = \mathbb{R}^\ell$:
\begin{equation}
d(x,y) = \sum^{\ell}_{i=1}|x_i-y_i|
\end{equation}
\columnbreak
\\
\highlight{Hamming distance} with $\mathbb{X} = \mathbb{D}_1 \times \mathbb{D}_2 \times \ldots \times \mathbb{D}_\ell$
\begin{equation}
d(x,y) = |\{\,i\,|\, x_i \neq y_i\}|
\end{equation}
\end{multicols}

\subsection{Nearest Neighbour Classifier}
Problem description: 'Learn' a function $f$, that associates a class $f(x) \in \mathbb{Y}$ with every $x \in \mathbb{X}$. Basically: $f: \mathbb{X} \rightarrow \mathbb{Y},x \mapsto y$. Function $f$ is provided with a dataset to learn from: $(x_1,y_1), \ldots, (x_m,y_m)$ with $m$ entries where each entry corresponds to $(x_i,f(x_i))$. $x_i$ is called data item and $y_i$ is called class $x_i$.

\highlight{k-Nearest Neighbour Classifier} uses $x \in \mathbb{X}$ as input.
\begin{enumerate}
	\item Find the $k$ nearest neighbours $x_{i_1},\ldots,x_{i_k}$ of x in $\{x_1,\ldots,x_m\}$
	\item Take a 'majority vote' of the class that appears the most among $y_{i_1},\ldots,y_{i_k}$
\end{enumerate}

Choosing a appropriate parameter $k$. When choosing $k=1$ the classifier tends to overhit. While with larger $k$ one might start to oversimplify. Obviously, the right $k$ is difficult to compute and depends heavily on the application.

\subsection{Learning Decision Trees}
A decision tree is a rooted tree with labelled notes and edges
\begin{itemize}
	\item Every internal node of the tree is labelled by an (input) feature
	\item Every edge is labeled by a value or range of values for the feature labelling its source node.
	\item Every leaf is labeled with an output value
\end{itemize}
Semantics: The function value for an input vector x is the value at the leaf of the unique path in the tree whose edges are labelled by the feature values in $x$.

Note: Computing a smallest decision tree for a given sets of examples is NP-Complete
\subsubsection{Greedy Algorithm for Building Decision Trees}
TODO: Greedy Algorithm for Building Decision Trees

\section{The Perceptron}
The Perceptron algorithm attempts tom find a linear classifier for a Boolean classification problem.
\subsection{Scalar product}
Two vectors, $\mathbf{x} = (x_1,\ldots,x_l)^T$, $\mathbf{y} = (y_1,\ldots,y_l)^T \in \mathbb{R}^\ell$ \mathmarginbox{$\langle \mathbf{x},\mathbf{y} \rangle$}
\begin{equation*}
\langle \mathbf{x},\mathbf{y} \rangle = \sum_{i=1}^{\ell} x_iy_i
\end{equation*}
\subsection{Euclidean norm}
Vector $\mathbf{x} \in \mathbb{R}^\ell$ \mathmarginbox{$\lVert \mathbf{x} \rVert$}
\begin{equation*}
\lVert \mathbf{x} \rVert = \sqrt{\langle \mathbf{x},\mathbf{x} \rangle} = \sqrt{ \sum_{i=1}^{\ell} x_i^2}
\end{equation*}
\subsection{Cauchy-Schwarz Inequality}
\begin{equation}
| \langle \mathbf{x},\mathbf{y} \rangle | \leq \lVert \mathbf{x} \rVert \cdot \lVert \mathbf{y} \rVert
\end{equation}

\subsection{Hyperplanes}
A (affine) \highlight{Hyperplane} in $\mathbb{R}^\ell$ ist an affine subspace of dimension $\ell -1$. The Hyperplane then describes the set of all solutions $a_1x_1 + \ldots + a_\ell x_\ell = b$ where $a_1,\ldots,a_\ell,b \in \mathbb{R}$. In short:  $\langle\mathbf{a}, \mathbf{x}\rangle - b=0$, or more formally: $P = \{x \in \mathbb{R}^\ell\,|\,\langle\mathbf{a}, \mathbf{x}\rangle = b\}$.

A \highlight{homogeneous hyperplane} is defined as a Hyperplane $P = \{x \in \mathbb{R}^\ell\,|\, \mscal{\mvec{a}, \mvec{x}} = b\}$ which is homogeneous if $b = 0$, or equivalently, if $0 \in P$ that's, the Hyperplane goes through the origin.
\subsection{Halfspaces}
A \highlight{Halfspace} in $\mathbb{R}^{\ell}$ is the set of all points on one side of a hyperplane. A Halfspace can described as the set of solutions $\mathbf{x}$ to an inequality $\langle\mathbf{a}, \mathbf{x}\rangle - b \geq 0$ where $\mathbf{a} \in \mathbb{R}^\ell$ and $b \in \mathbb{R}$. Or more formally: $H = \{x \in \mathbb{R}^\ell \, | \, \langle\mathbf{a}, \mathbf{x}\rangle - b \geq 0\}$

A halfspace is a \highlight{homogeneous Halfspace} as follows: $H = \{x \in \mathbb{R}^\ell \, | \, \mscal{\mvec{a}, \mvec{x}} \geq 0\}$. So basically a halfspace with $b = 0$.

\subsection{Linear Classification}
The \highlight{Boolean Classification} is a classification problem with the instance space $\mathbb{X} = \{+1,-1\}$. The goal is to learn a unknown target function $f:\mathbb{R}^\ell \rightarrow \{+1,-1\}$.

The input for the learning function is a sequence of data items (so called training sequence):
\begin{equation}
S=((\mathbf{x}_{1}, y_{1}), \ldots,(\mathbf{x}_{m}, y_{m})) \in \mathbb{R}^{\ell} \times\{+1,-1\}
\end{equation}

The hypothesis space ($\mathcal{H}$) consists of \highlight{linear separators}, which are functions $h:\mathbb{R}^\ell \rightarrow \mathbb{R}$ in the form of:
\begin{equation} 
h(\mvec{x})=\operatorname{sgn}(\mscal{\mvec{w}, \mvec{x}}- b)=
\begin{cases}
+1 & \, \text {if }\mscal{\mvec{w}, \mathbf{x}} - b>0 \\
0 & \,\text {if }\mscal{\mvec{w}, \mathbf{x}} - b=0 \\
-1 & \,\text {if }\mscal{\mvec{w}, \mathbf{x}} - b<0
\end{cases}
\end{equation}
with $w \in \mathbb{R}^\ell$, the weight vector and $b\in \mathbb{R}^\ell$ the bias.

Because the mismatching target function range of $\{+1,0,-1\}$ and $\{+1,-1\}$ is redefined as follows:
\begin{equation*}
f(\mathbf{x})=\left\{\begin{array}{ll}{+1} & {\text { if }\langle\mathbf{w}, \mathbf{x}\rangle- b \geq 0} \\ {-1} & {\text { if }\langle\mathbf{w}, \mathbf{x}\rangle- b<0}\end{array}\right.
\end{equation*}

A hypothesis $h$ is an \highlight{consistent hypothesis} with a training sequence $S = ((\mathbf{x}_{1}, y_{1}), \ldots,(\mathbf{x}_{m}, y_{m}))$ if $h(\mathbf{x}_i) = y_i$ for all $i \in [m]$.

\subsubsection{non-bias Linear Classification}
A \highlight{homogeneous linear separator} is a function, such that $\mathbf{x} \mapsto \mscal{\mvec{w},\mvec{x}}$. This implies, the linear separator goes through the origin of the coordinate system.	
\begin{theorem}[Properties of non-bias Linear Classification]{important}
Let $S=((\mvec{x}_{1}, y_{1}), \ldots,(\mvec{x}_{m}, y_{m}))$ with $\mvec{x}_{i}=(x_{i _1}, \ldots, x_{i_\ell}) \in \mathbb{R}^\ell$ and $y_{i} \in\{-1,1\}$ and $\mvec{w}=(w_{1}, \ldots, w_\ell) \in \mathbb{R}^\ell$ and $b \in \mathbb{R}$. Then the following are equivalent.
\begin{enumerate}
	\item $\mvec{x} \mapsto \operatorname{sgn}(\mscal{\mvec{w}, \mvec{x}}- b)$ is a linear separator consistent with $S$
	\item $\mvec{x}^{\prime} \mapsto \operatorname{sgn}(\mscal{\mvec{w}^\prime, \mvec{x}^\prime})$ is a homogeneous linear separator consistent with $S^\prime=((\mvec{x}_1^\prime, y_1), \ldots,(\mvec{x}_m^\prime, y_m))$,  where $\mvec{x}_i^\prime :=(x_{i_1}, \ldots, x_{i_\ell}, 1) $ and $\mvec{w}^\prime=(w_{1}, \ldots, w_\ell,-b)$
\end{enumerate}
\end{theorem}
\subsubsection{Normalized Linear Classification}
Intuition: as the name suggests informally, we simply divide all $\mvec{x}$ in the training sequence by the length of the largest $\mvec{x}$ vector. Thus, the largest vector then has a length of at most 1.

Suppose we have a training sequence $S=((\mvec{x}_1, y_1), \ldots,(\mvec{x}_{m}, y_{m}))$ with $\mvec{x}_{i}=(x_{i_1}, \ldots, x_{i_\ell}) \in \mathbb{R}^\ell$ and $y_{i} \in\{-1,1\}$. We can apply the following transformation to all datapoints $\mvec{x}_i$
\begin{equation*}
\mvec{x}_{i} \mapsto \widehat{\mvec{x}}_{\mvec{i}} :=\frac{\mvec{x}_{i}^{\prime}}{\max _{1 \leq j \leq m}\lVert\mvec{x}_{j}^{\prime}\rVert}
\end{equation*}
Resulting in the \highlight{normalized training sequence} $\widehat{S}=((\widehat{\mvec{x}}_{1}, y_{1}), \ldots,(\widehat{\mvec{x}}_{m}, y_{m}))$. Additionally $\widehat{S}$ has following properties: 
\begin{enumerate}
	\item $\widehat{S}$ has a homogeneous linear separator if and only if $S$ has alinear separator
	\item $0 < \lVert\widehat{\mvec{x}}_{\mathrm{i}}\rVert \leq 1 $ for all $ i \in [m]$
\end{enumerate}
\subsubsection{Linear Seperator Margin}
The \highlight{margin of a linear seperator} is the orthogonal distance of the nearest point to the linear seperator function $h$. Formally: the margin of $h$ with respect to the target sequence $S$ is
\begin{equation*}
\min _{(x, y) \in S}|\mscal{\mvec{w}, \mvec{x}}|
\end{equation*} 
\begin{theorem}{}
	Let S  be a normalized sequence of examples such that there is a homogeneous linear separator consistent with $S$ of margin $\gamma$. Then the perceptron algorithm applied to $S$ finds a linear separator after at most $\frac{1}{\gamma^2}$updates of $w$.
\end{theorem}
\begin{proof}{}
	See Page 1.46 in the slidedeck for Kap1
\end{proof}
\subsubsection{The Perceptron Algorithm}
\subsubsection{Linear Separation for Nonlinear Problems}
\section{Neural Networks}
\section{k-Means Clustering}
\subsection{Centroid Based Clustering}
\subsection{k-Means Clustering Algorithm}
