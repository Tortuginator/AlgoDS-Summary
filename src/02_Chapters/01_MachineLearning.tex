%%% File encoding is ISO-8859-1 (also known as Latin-1)
%%% You can use special characters just like �,� and �
\include{amsmath}
\chapter{Machine Learning Basics}

\section{Definitions}
All the following definitions are specific to 'Machine Learning' and might change throughout the summary.


\highlight{Data} is viewed as a collection of data items (or dataset). A \highlight{data item} is represented by a so-called feature vector. The \highlight{feature vector} basically a list of features and attributes in form of a vector 
\subsection{Domains}
The \symbdefbox{Domain}{$\mathbb{D}$} is a set containing the range of all possible values for a single feature. The \symbdefbox{Instance Space}{$\mathbb{X}$} is the Cartesian product of the domains of all features in the feature vector. The number of features is the dimension of the instance space (or the number of entries in the feature vector). 
\example{The domain space can be something like $\{true,false\}$ or $\mathbb{N}$, aso. The instance space of the domain spaces is then (informally) $\mathbb{D}_1\times \mathbb{D}_2 \times \ldots \times \mathbb{D}_l$}
Assuming $n$ is the number of data items, and $l$ the number of features the whole dataset can be viewed as an $(n \times l )$-matrix over the instance space.

\section{Forms of Learning}
\highlight{Unsupervised learning} tries to detect patterns in data with, hence the name, no feedback, supplied the Learning algorithm. This technique is mostly used for clustering.

\highlight{Reinforcement Learning} tries to find actions that maximise reward. The feedback to the learning algorithm is provided in form of a reward (or punishment)

In \highlight{Supervised learning} one tries to approximate a function by training on input-output pairs. \highlight{Classification} has a finite-valued function and tries to predict values for future inputs. \highlight{Regression} has a numerical function and tries to predict the expected values for future inputs. Supervised learning can be differentiated into two different set-ups. First, \highlight{Batch Learning}, in which all examples are supplied at once and the learner has to come up with a hypothesis. Where as in \highlight{Online Learning}, examples are given 'on-the-fly' once at a time and the initial hypothesis gets improved over time.

\highlight{Semi-supervised Learning} is similar to supervised learning, but takes only a few and possibly faulty examples and tries to make the best of the possibly small and faulty dataset.

%TODO: Add Active/Passive Learning

\section{Hypothesis Space}
In Supervised learning, the unknown target function $h$ is choosen from the \symbdefbox{hypothesis space}{$\mathcal{H}$}, hence $h \in \mathcal{H}$. The Hypothesis space contains all linear functions, all polynomials or all polynomials of a pre-scribed degree, and all functions that can be described by a decision tree.

A learning problem is \highlight{Realisable} if the target function $h$ is in the Hypothesis Space.
\example{Assuming a target function $h(x):= a \cdot \operatorname{sin}(x) + bx + c$. Then, if $\mathcal{H}$ only contains polynomials, the learning problem is \textbf{not realizable}. But, assuming $\mathcal{H}$ contains linear functions or polynomials in $x$ and $\operatorname{sin}(x)$, the learning problem \textbf{is realisable}.}

\section{Validation}
The goal of the learning algorithm is to produce a hypothesis that generalizes well, that is, approximates the target function well ion the all data points (and not only those in the training set). To evaluate how well a hypothesis generalizes we can evaluate it against some test set. A \highlight{test set} is generates by splitting the examples into a test set and training set.

The empirical observation that simpler hypothesis tend to generalise better is picked up by \highlight{Occam's Razor} which states, that the simplest hypothesis which is consistent with the data should be choosen.

\highlight{Overfitting} is the phenomenon, which occurs when the leaner tries to match the training examples as exactly as possible which leads to complex hypotheses that generalize badly. Thus, to avoid overfitting, it is often better to choose a simple hypothesis even if it doesn't match the data exactly.

\section{Nearest Neighbour Learning}
The underlying idea of this simple learning algorithm is to predict the value of a function at point $x$ by looking at known values of points close to the given one and assume the value of $x$ is similar. 
\subsection{Metric}
A \symbdefbox{metric}{$d$} on $\mathbb{X}$ is a function $d: \mathbb{X}^2 \longrightarrow  \mathbb{R}$ such that for all $x,y,z \in \mathbb{X}$

\newlength\q
\setlength\q{\dimexpr .33\textwidth -2\tabcolsep}
\begin{tabular}{p{\q}|p{\q}|p{\q}}
	\highlight{Nonnegativity} & \highlight{Symmetry} & \highlight{Triangle Inequality}\\
	$\begin{aligned} &d(x,y) \geq 0 \\	\texttt{and }  &d(x,y) = 0 \Longleftrightarrow x=y;\end{aligned}$ & 	$d(x,y) = d(y,x)$ &  $d(x,z) \leq d(x,y) + d(y,z)$\\
\end{tabular}


\subsection{Metric Spaces}
Suppose $\mathbb{X}$ is the instance space, and $d$ be some metric on $\mathbb{X}$. Then we have a \symbdefbox{Metric Space}{$(\mathbb{X},d)$}.

\subsubsection{Common distance metrics}
The distance between points $x$ and $y$ is denoted as $d(x,y)$\mathmarginbox{$d(x,y)$}.
\setlength{\columnseprule}{0.4pt}
\begin{multicols}{2}
\highlight{Euclidean distance} with $\mathbb{X} = \mathbb{R}^\ell$:
\begin{equation}
d(x,y) = \sqrt{\sum^{\ell}_{i=1} (x_i - y_i)^2}
\end{equation}

\highlight{Manhattan distance} with $\mathbb{X} = \mathbb{R}^\ell$:
\begin{equation}
d(x,y) = \sum^{\ell}_{i=1}|x_i-y_i|
\end{equation}
\columnbreak
\\
\highlight{Hamming distance} with $\mathbb{X} = \mathbb{D}_1 \times \mathbb{D}_2 \times \ldots \times \mathbb{D}_\ell$
\begin{equation}
d(x,y) = |\{\,i\,|\, x_i \neq y_i\}|
\end{equation}
\end{multicols}

\subsection{Nearest Neighbour Classifier}
Problem description: 'Learn' a function $f$, that associates a class $f(x) \in \mathbb{Y}$ with every $x \in \mathbb{X}$. Basically: $f: \mathbb{X} \rightarrow \mathbb{Y},x \mapsto y$. Function $f$ is provided with a dataset to learn from: $(x_1,y_1), \ldots, (x_m,y_m)$ with $m$ entries where each entry corresponds to $(x_i,f(x_i))$. $x_i$ is called data item and $y_i$ is called class $x_i$.

\highlight{k-Nearest Neighbour Classifier} uses $x \in \mathbb{X}$ as input.
\begin{enumerate}
	\item Find the $k$ nearest neighbours $x_{i_1},\ldots,x_{i_k}$ of x in $\{x_1,\ldots,x_m\}$
	\item Take a 'majority vote' of the class that appears the most among $y_{i_1},\ldots,y_{i_k}$
\end{enumerate}

Choosing an appropriate parameter $k$: When choosing $k=1$ the classifier tends to overhit, while with larger $k$ one might start to oversimplify. Obviously, the optimal value for $k$ is difficult to determine and depends heavily on the application.

\subsection{Learning Decision Trees}
A decision tree is a rooted tree with labelled notes and edges
\begin{itemize}
	\item Every internal node of the tree is labelled by an (input) feature
	\item Every edge is labeled by a value or range of values for the feature labelling its source node.
	\item Every leaf is labeled with an output value
\end{itemize}
Semantics: The function value for an input vector x is the value at the leaf of the unique path in the tree whose edges are labelled by the feature values in $x$.

Note: Computing a smallest decision tree for a given sets of examples is NP-Complete
\subsubsection{Greedy Algorithm for Building Decision Trees}
Input: Set $\mathcal{A}$ of features, set $\mathcal{S}$ of examples\\
Objective: Compute decision tree $t$

\begin{algorithmic}
	\If{$\mathcal{S} = \emptyset$}
		\State{create leaf $t$ with arbitrary value}
	\ElsIf{all samples in $\mathcal{S}$ have the same $y$-value}
		\State{create leaf $t$ with that $y$-value}
	\Else
		\State{choose feature $A \in \mathcal{A}$ that discriminates best between examples in $\mathcal{S}$}
		\State{create new node $t$ with feature $A$}
		\State{partition examples in S according to their $A$-value into parts $S_1, ...,S_m$}
		\State{recursively call algorithm on $\mathcal{A}\backslash\{A\}$ and the $S_i$ and attach resulting trees $t_i$ as children to $t$}
	\EndIf\\
	\Return{$t$}
\end{algorithmic}

\section{The Perceptron}
The Perceptron algorithm attempts to find a linear classifier for a Boolean classification problem.
\subsection{Scalar product}
Two vectors, $\mathbf{x} = (x_1,\ldots,x_l)^T$, $\mathbf{y} = (y_1,\ldots,y_l)^T \in \mathbb{R}^\ell$ \mathmarginbox{$\langle \mathbf{x},\mathbf{y} \rangle$}
\begin{equation*}
\langle \mathbf{x},\mathbf{y} \rangle = \sum_{i=1}^{\ell} x_iy_i
\end{equation*}
\subsection{Euclidean norm}
Vector $\mathbf{x} \in \mathbb{R}^\ell$ \mathmarginbox{$\lVert \mathbf{x} \rVert$}
\begin{equation*}
\lVert \mathbf{x} \rVert = \sqrt{\langle \mathbf{x},\mathbf{x} \rangle} = \sqrt{ \sum_{i=1}^{\ell} x_i^2}
\end{equation*}
\subsection{Cauchy-Schwarz Inequality}
\begin{equation}
| \langle \mathbf{x},\mathbf{y} \rangle | \leq \lVert \mathbf{x} \rVert \cdot \lVert \mathbf{y} \rVert
\end{equation}

\subsection{Hyperplanes}
A (affine) \highlight{Hyperplane} in $\mathbb{R}^\ell$ is an affine subspace of dimension $\ell -1$. It can be described as the set of all solutions $\mathbf{x} \in \mathbb{R}^\ell$ to the equation $a_1x_1 + \ldots + a_\ell x_\ell = b$ where $a_1,\ldots,a_\ell,b \in \mathbb{R}$. In short:  $\langle\mathbf{a}, \mathbf{x}\rangle - b=0$, or more formally: $P = \{x \in \mathbb{R}^\ell\,|\,\langle\mathbf{a}, \mathbf{x}\rangle = b\}$.

A Hyperplane $P = \{x \in \mathbb{R}^\ell\,|\, \mscal{\mvec{a}, \mvec{x}} = b\}$ is \highlight{homogeneous} if $b = 0$, or equivalently, if $0 \in P$. That's, the Hyperplane goes through the origin.
\subsection{Halfspaces}
A \highlight{Halfspace} in $\mathbb{R}^{\ell}$ is the set of all points on one side of a Hyperplane. A Halfspace can be described as the set of solutions $\mathbf{x} \in \mathbb{R}^\ell$ to an inequality $\langle\mathbf{a}, \mathbf{x}\rangle - b \geq 0$ where $\mathbf{a} \in \mathbb{R}^\ell$ and $b \in \mathbb{R}$. Or more formally: $H = \{x \in \mathbb{R}^\ell \, | \, \langle\mathbf{a}, \mathbf{x}\rangle - b \geq 0\}$

A Halfspace is a \highlight{homogeneous Halfspace} as follows: $H = \{x \in \mathbb{R}^\ell \, | \, \mscal{\mvec{a}, \mvec{x}} \geq 0\}$. So basically a halfspace with $b = 0$.

\subsection{Linear Classification}
The \highlight{Boolean Classification} is a classification problem with the domain space $\mathbb{D} = \{+1,-1\}$. The goal is to learn an unknown target function $f:\mathbb{R}^\ell \rightarrow \{+1,-1\}$.

The input for the learning function is a sequence of data items (so called training sequence):
\begin{equation}
S=((\mathbf{x}_{1}, y_{1}), \ldots,(\mathbf{x}_{m}, y_{m})) \in \mathbb{R}^{\ell} \times\{+1,-1\}
\end{equation}

The hypothesis space ($\mathcal{H}$) consists of \highlight{linear separators}, which are functions $h:\mathbb{R}^\ell \rightarrow \mathbb{R}$ in the form of:
\begin{equation} 
h(\mvec{x})=\operatorname{sgn}(\mscal{\mvec{w}, \mvec{x}}- b)=
\begin{cases}
+1 & \, \text {if }\mscal{\mvec{w}, \mvec{x}} - b>0 \\
0 & \,\text {if }\mscal{\mvec{w}, \mvec{x}} - b=0 \\
-1 & \,\text {if }\mscal{\mvec{w}, \mvec{x}} - b<0
\end{cases}
\end{equation}
with $w \in \mathbb{R}^\ell$, the weight vector and $b\in \mathbb{R}^\ell$ the bias.

Because of the mismatching target function range of $\{+1,0,-1\}$ and the domain space of the Boolean Classification $\{+1,-1\}$ the linear separator function is redefined as follows:
\begin{equation}
f(\mathbf{x})=\left\{\begin{array}{ll}{+1} & {\text { if } \mscal{\mvec{w}, \mvec{x}} - b \geq 0} \\ {-1} & {\text { if }\mscal{\mvec{w}, \mvec{x}} - b<0}\end{array}\right.
\end{equation}

A hypothesis $h$ is a \highlight{consistent hypothesis} if for a training sequence $S = ((\mathbf{x}_{1}, y_{1}), \ldots,(\mathbf{x}_{m}, y_{m}))$ the following applies: $h(\mathbf{x}_i) = y_i$ for all $i \in [m]$.

\subsubsection{Non-bias Linear Classification}
A \highlight{homogeneous linear separator} is a function, such that $\mathbf{x} \mapsto \mscal{\mvec{w},\mvec{x}}$. The removal of the bias ($b$) implies, the linear separator goes through the origin of the coordinate system.	
\begin{theorem}[Properties of non-bias Linear Classification]{important}
Let $S=((\mvec{x}_{1}, y_{1}), \ldots,(\mvec{x}_{m}, y_{m}))$ with $\mvec{x}_{i}=(x_{i _1}, \ldots, x_{i_\ell}) \in \mathbb{R}^\ell$ and $y_{i} \in\{-1,1\}$ and $\mvec{w}=(w_{1}, \ldots, w_\ell) \in \mathbb{R}^\ell$ and $b \in \mathbb{R}$. Then the following are equivalent.
\begin{enumerate}
	\item $\mvec{x} \mapsto \operatorname{sgn}(\mscal{\mvec{w}, \mvec{x}}- b)$ is a linear separator consistent with $S$
	\item $\mvec{x}^{\prime} \mapsto \operatorname{sgn}(\mscal{\mvec{w}^\prime, \mvec{x}^\prime})$ is a homogeneous linear separator consistent with $S^\prime=((\mvec{x}_1^\prime, y_1), \ldots,(\mvec{x}_m^\prime, y_m))$,  where $\mvec{x}_i^\prime :=(x_{i_1}, \ldots, x_{i_\ell}, 1) $ and $\mvec{w}^\prime=(w_{1}, \ldots, w_\ell,-b)$
\end{enumerate}
\end{theorem}
\subsubsection{Normalized Linear Classification}
Intuition: as the name suggests informally, we simply divide all $\mvec{x}$ in the training sequence by the length of the largest $\mvec{x}$ vector. Therefore, the largest vector then has a length of at most 1.

Suppose we have a training sequence $S=((\mvec{x}_1, y_1), \ldots,(\mvec{x}_{m}, y_{m}))$ with $\mvec{x}_{i}=(x_{i_1}, \ldots, x_{i_\ell}) \in \mathbb{R}^\ell$ and $y_{i} \in\{-1,1\}$. We can apply the following transformation to all datapoints $\mvec{x}_i$
\begin{equation}
\mvec{x}_{i} \mapsto \widehat{\mvec{x}}_{\mvec{i}} :=\frac{\mvec{x}_{i}^{\prime}}{\max _{1 \leq j \leq m}\lVert\mvec{x}_{j}^{\prime}\rVert}
\end{equation}
Resulting in the \highlight{normalized training sequence} $\widehat{S}=((\widehat{\mvec{x}}_{1}, y_{1}), \ldots,(\widehat{\mvec{x}}_{m}, y_{m}))$. Additionally $\widehat{S}$ has following properties: 
\begin{enumerate}
	\item $\widehat{S}$ has a homogeneous linear separator if and only if $S$ has a linear separator
	\item $0 < \lVert\widehat{\mvec{x}}_{\mathrm{i}}\rVert \leq 1 $ for all $ i \in [m]$
\end{enumerate}
\subsubsection{Linear Seperator Margin}
The \highlight{margin of a linear seperator} is the orthogonal distance of the nearest point to the linear separator function $h$. Formally: the margin of $h$ with respect to the target sequence $S$ is
\begin{equation}
\min_{(x, y) \in S}|\mscal{\mvec{w}, \mvec{x}}|
\end{equation} 
\begin{theorem}{}
	Let S  be a normalized sequence of examples such that there is a homogeneous linear separator consistent with $S$ of margin $\gamma$. Then the perceptron algorithm applied to $S$ finds a linear separator after at most $\frac{1}{\gamma^2}$updates of $w$.
\end{theorem}
\begin{proof}{}
	See Page 1.46 in the slidedeck for Kap1
\end{proof}
\subsubsection{The Perceptron Algorithm}
Input: Normalized training sequence $S$\\
Objective: Compute weight vector $\mvec{w}$ such that the hypothesis $x \mapsto \operatorname{sgn}(\mscal{\mvec{x},\mvec{w}})$ is consistent with $S$

%TODO: FIX LAYOUT
\begin{algorithmic}
	\State $w\gets 0$
	\Repeat 
		\ForAll{$(x,y) \in S$} 
		\If {$\operatorname{sgn}(\mscal{\mvec{x},\mvec{w}}) \neq y$}
			\State $w \gets w + yx$
		\EndIf
		\EndFor
	\Until{$\operatorname{sgn}(\mscal{\mvec{x},\mvec{w}}) = y$ for all $(x,y) \in S$}
\end{algorithmic}
\subsubsection{Linear Separation for Nonlinear Problems}
Suppose the instance space is $\mathbb{R}^2$ and the hypothesis space consists of the quadratic separators if the form:
\begin{equation}
(x_1, x_2) \mapsto \operatorname{sgn}(a_1 x_1^2+a_2 x_1 x_2+a_3 x_2^2+a_4 x_1+a_5 x_2+a_6)
\end{equation}
To learn the parameters $a_1,\ldots,a_6$, we define a transformation $\tau :\mathbb{R}^2 \rightarrow \mathbb{R}^5$ by
\begin{equation}
\tau(x_1,x_2) = (x_1^2,x_1x_2,x_2^2,x_1,x_2)
\end{equation}
Then quadratic separators for a training sequence
\begin{equation}
S=((\mathrm{x}_{1}, y_{1}), \ldots,(\mathrm{x}_{m}, y_{m})) \in \mathbb{R}^{2} \times\{+1,-1\}
\end{equation}
correspond to linear separator for the transformed training sequence
\begin{equation}
\tau(S) :=((\tau(\mathrm{x}_{1}), y_{1}), \ldots,(\tau(\mathrm{x}_{m}), y_{m})) \in \mathbb{R}^{5} \times\{+1,-1\}
\end{equation}
\section{Neural Networks}
TODO: Add this section. Maybe later, but not that important, since the section on the slides is very broad
\section{$k$-Means Clustering}
This is the only example of unsupervised learning covered by the lecture.\\
Task: Put a collection of data points into $k$ clusters whereby the intra-cluster distances are to be minimized and the inter-cluster distances are to be maximized. The number of clusters $k$ is fixed in advance.\\
Again, we take $\mathbb{R}^\ell$ as the instance space and use the Euclidean metric for distance measurements.\\

\subsection{Centroid Based Clustering}
In centroid based clustering each cluster in represented by its centroid $z$ (a.k.a. "centre of a mass" or "mean"), i.e. it is represented by the point that minimizes the squared distance to all points in the cluster. Each data point $x$ will be associated with the cluster whose centroid is closest to $x$.\\

Instance: Points $x_1, ..., x_n \in \mathbb{R}^\ell$, number $k \in \mathbb{N}$\\
Problem: Find points $z^1,...,z^k \in \mathbb{R}^\ell$ and a partition $C^1,...,C^k$ of $\{x_1,...,x_n\}$ that minimizes\\
\begin{equation}
	\sum_{j=1}^k \sum_{x \in C^j} \lVert x-z^j \rVert ^2
\end{equation}

\begin{theorem}{}
	(1) The Centroid Clustering problem is NP-hard, even if either the dimension $\ell$ or the cluster\\
	number $k$ are fixed to be 2.\\
	(2) If both $k$ and $\ell$ are fixed, the problem can be solved in polynomial time.
\end{theorem}
\begin{proof}{}
	was omitted
\end{proof}

\subsection{$k$-Means Clustering Algorithm}
Input: $x_1,\ldots,x_n \in \mathbb{R}^\ell$, $k \in \mathbb{N}$\\
Objective: c.f. 1.8.1\\

\begin{algorithmic}
	\State{Choose initial "centroids" $z^1,...,z^k$ (for example randomly)}
	\Repeat 
		\State{$C^j \gets \emptyset \; \forall \, j \in [k]$}
	\For{$i \gets 1$ to $n$}
		\State{$j \gets \arg \min_j \lVert x_i - z^j \rVert$} \hspace{1cm} (if there is a tie, choose the smallest $j$)
		\State{add $x_i$ to $C^j$}
	\EndFor
	\State{$z^j \gets \frac{\sum_{x \in C^j} x}{|C^j|} \forall j \in [k]$}
	\Until{$C^1,...,C^k$ no longer change}\\
	\Return{$C^1,...,C^k$}
\end{algorithmic}

\begin{theorem}{}
The $k$-Means algorithm always halts in a finite number of steps.
\end{theorem}
\begin{proof}{}
	See Page 1.56 in the slidedeck for Kap1
\end{proof}

Remarks on the algorithm:
\begin{itemize}
	\item{The number of steps the $k$-Means algortihm takes until it halts may be exponential in the number $n$ of input points - however in practice the algorithm usually converges quickly}
	\item{The $k$-Means algorithm does not necessarily conpute an optimal solution for the Centroid Clustering problem}
	\item{The clustering returned by the $k$-Means algorithm may depend on the choice of the inituial centroids $z^1,\ldots,z^k$}
\end{itemize}